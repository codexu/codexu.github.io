import{_ as a,c as o,o as t,ae as r}from"./chunks/framework.BtEkjhVd.js";const h=JSON.parse('{"title":"AI Configuration","description":"","frontmatter":{},"headers":[],"relativePath":"en/settings/model-config.md","filePath":"en/settings/model-config.md"}'),i={name:"en/settings/model-config.md"};function n(l,e,s,d,p,c){return t(),o("div",null,e[0]||(e[0]=[r('<h1 id="ai-configuration" tabindex="-1">AI Configuration <a class="header-anchor" href="#ai-configuration" aria-label="Permalink to &quot;AI Configuration&quot;">​</a></h1><p>NoteGen integrates mainstream AI model services and supports custom configurations to meet different needs.</p><p><img src="https://s2.loli.net/2025/05/26/S9C58QMryevcbJA.png" alt="1.png"></p><h2 id="configuration" tabindex="-1">Configuration <a class="header-anchor" href="#configuration" aria-label="Permalink to &quot;Configuration&quot;">​</a></h2><h3 id="model-provider" tabindex="-1">Model Provider <a class="header-anchor" href="#model-provider" aria-label="Permalink to &quot;Model Provider&quot;">​</a></h3><p>Here you can choose the model provider. If your service provider is not listed, please click the add button to customize and add a configuration.</p><p>Click copy to duplicate the same configuration, useful for using different models from the same service provider.</p><div class="tip custom-block"><p class="custom-block-title">TIP</p><p>For locally deployed models like Ollama, LM Studio, etc., if they don&#39;t work properly after configuration, please check if cross-domain access is enabled.</p></div><h3 id="baseurl" tabindex="-1">BaseURL <a class="header-anchor" href="#baseurl" aria-label="Permalink to &quot;BaseURL&quot;">​</a></h3><p>Note that you only need to configure up to the version number, e.g., <a href="https://api.openai.com/v1" target="_blank" rel="noreferrer">https://api.openai.com/v1</a>. The suffix will be added automatically. The same applies to other providers as well. Please do not include <code>chat/completions</code> in your input.</p><h3 id="api-key" tabindex="-1">API Key <a class="header-anchor" href="#api-key" aria-label="Permalink to &quot;API Key&quot;">​</a></h3><p>Enter your corresponding key here. For Ollama, LM Studio, etc., you can fill in anything.</p><h3 id="model" tabindex="-1">Model <a class="header-anchor" href="#model" aria-label="Permalink to &quot;Model&quot;">​</a></h3><p>Generally, once the BaseURL and API Key are configured correctly, available models will be automatically retrieved and presented in a dropdown list. If they cannot be retrieved, you can enter them manually.</p><h3 id="model-type" tabindex="-1">Model Type <a class="header-anchor" href="#model-type" aria-label="Permalink to &quot;Model Type&quot;">​</a></h3><p>Select the model type to distinguish different models. Choosing the wrong model type may cause it to not function properly.</p><h3 id="temperature" tabindex="-1">Temperature <a class="header-anchor" href="#temperature" aria-label="Permalink to &quot;Temperature&quot;">​</a></h3><p>The sampling temperature to use, between 0 and 2. Higher values (like 0.8) will make the output more random, while lower values (like 0.2) will make it more focused and deterministic. We generally recommend altering this or top_p but not both.</p><h3 id="top-p" tabindex="-1">Top P <a class="header-anchor" href="#top-p" aria-label="Permalink to &quot;Top P&quot;">​</a></h3><p>An alternative to temperature sampling called nucleus sampling, where the model considers the results of tokens with top_p probability mass. So 0.1 means only considering tokens comprising the top 10% probability mass. We generally recommend altering this or temperature but not both.</p>',20)]))}const m=a(i,[["render",n]]);export{h as __pageData,m as default};
